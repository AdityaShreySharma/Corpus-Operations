{"cells":[{"cell_type":"markdown","metadata":{"id":"yENWO4rt8Rmv"},"source":["Importing all the Corpora included in NLTK using the following commands :-"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4oXxbxx-_ai"},"outputs":[],"source":["#importing nltk\n","import nltk\n","\n","#installing all the custom corpora\n","nltk.download()"]},{"cell_type":"markdown","metadata":{"id":"dWcbBxVV8h4S"},"source":["Python code to create the NLTK directory and verify that it is in the list of known paths specified by nltk.data.path "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dY6Vvj718muU"},"outputs":[],"source":["# importing libraries\n","import os, os.path\n","\n","# using the given path\n","path = os.path.expanduser(\"~/nltk_data\")\n","\n","# checking\n","if not os.path.exists(path):\n","    os.mkdir(path)\n","print(\"\\n\")\n","print (\"Path Exists = \", os.path.exists(path))\n","import nltk.data\n","print (\"Path Exists in NLTK = \", path in nltk.data.path)"]},{"cell_type":"markdown","metadata":{"id":"NB9BB9YQ9dRS"},"source":["--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"hqWFR_5m8sRb"},"source":["1) WORD FREQUENCY IDENTIFICATION :-"]},{"cell_type":"markdown","metadata":{"id":"78NBkvfC9M4D"},"source":["a) Using the UDHR (Universal Declaration of Human Rights) Corpus to examine the differences in word lengths for a selection of languages included in this corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"651RGjyl9aSa"},"outputs":[],"source":["#importing nltk\n","import nltk \n","\n","#downloading the udhr package\n","nltk.download('udhr')\n","\n","#importing the corpus udhr\n","from nltk.corpus import udhr\n","\n","#selected languages for testing\n","languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n","\n","#examining the difference in word lengths\n","cfd = nltk.ConditionalFreqDist(\n","    (lang, len(word))\n","    for lang in languages\n","    for word in udhr.words(lang + '-Latin1'))\n","\n","#plotting the Samples vs Counts curve\n","cfd.plot(cumulative=True)"]},{"cell_type":"markdown","metadata":{"id":"_L5MTfy0_PYF"},"source":["b) Using the WebText Corpus to count the number of words in this corpus having a frequency of greater than 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFZ69tkZ_ULj"},"outputs":[],"source":["#importing nltk \n","import nltk\n","\n","#downloading the webtext package\n","nltk.download('webtext')\n","\n","#importing the webtext corpus\n","from nltk.corpus import webtext\n","wt_words = webtext.words('firefox.txt')\n","data_analysis = nltk.FreqDist(wt_words)\n","\n","#taking the specific words only if their frequency is greater than 3.\n","filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n","data_analysis = nltk.FreqDist(filter_words)\n","\n","#plotting the Words vs Counts curve\n","data_analysis.plot(25, cumulative=False)"]},{"cell_type":"markdown","metadata":{"id":"y5iMQ3hV_fP0"},"source":["c) Using the Names Corpus, which contains 8000 first names categorized by gender, to find names which appear in both files, i.e. names that are ambiguous for gender. The male and female names are stored in separate files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQiTBV7o_m09"},"outputs":[],"source":["#importing nltk \n","import nltk\n","\n","# downloading the names package \n","nltk.download('names')\n","\n","#importing the names corpus and storing it in a variable \n","names = nltk.corpus.names\n","\n","#comparing the gender-wise ambiguous names\n","cfd = nltk.ConditionalFreqDist(\n","    (fileid, name[-1])\n","    for fileid in names.fileids()\n","    for name in names.words(fileid))\n","\n","#plotting the Samples vs Counts curve\n","cfd.plot()"]},{"cell_type":"markdown","metadata":{"id":"09c5M19m_4vc"},"source":["--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"XPex2gGH_7Wg"},"source":["2) CORPUS ANNOTATION (POS TAGGING) :-"]},{"cell_type":"markdown","metadata":{"id":"XK4dJuBqABY2"},"source":["a) Using a section of the Gutenberg Corpus to add POS tags for each individual word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzy9TVkuANwg"},"outputs":[],"source":["#importing the required corpora and dependencies\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('gutenberg')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n","from nltk.corpus import gutenberg\n","\n","#using english stop words\n","stop_words = set(stopwords.words('english'))\n","\n","#importing the file \"bible-kjv.txt\" from the gutenberg corpus\n","txt = gutenberg.raw(\"bible-kjv.txt\")  \n","\n","#tokenizing the text file from the corpus\n","tok = sent_tokenize(txt)\n","\n","#extracting a section out of the file\n","tokenize = tok[5:15]\n","\n","#tagging \n","for i in tokenize:\n","    wordsList = nltk.word_tokenize(i)\n","    wordsList = [w for w in wordsList if not w in stop_words] \n","    tagged = nltk.pos_tag(wordsList)\n","    print(tagged)"]},{"cell_type":"markdown","metadata":{"id":"N60jlyLNAlka"},"source":["b) Using the Brown Corpus to find out the words that are highly ambiguous as to their part of speech tag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhT3PvQ3AoTV"},"outputs":[],"source":["#importing the required dependencies and corpus\n","import nltk\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","from nltk.corpus import brown\n","\n","#tagging the words from the news category\n","brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n","\n","#testing for ambiguity among the POS tags\n","data = nltk.ConditionalFreqDist((word.lower(), tag)\n","for (word, tag) in brown_news_tagged)\n","for word in sorted(data.conditions()):\n","    if len(data[word]) > 3:\n","        tags = [tag for (tag, _) in data[word].most_common()]\n","        print(word, ' '.join(tags))"]},{"cell_type":"markdown","metadata":{"id":"iCOPf98mA5_i"},"source":["--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"9GZfm01uA9Gp"},"source":["3) CREATING PARSE TREES USING THE TREEBANK CORPUS :-"]},{"cell_type":"markdown","metadata":{"id":"JZ0qhUJYP_3M"},"source":["a) Parse Tree 1 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9ZOdWsTBMEO"},"outputs":[],"source":["#setting the display name and $DISPLAY environment variable \n","import os\n","os.system('Xvfb :1 -screen 0 1600x1200x16  &')\n","os.environ['DISPLAY']=':1.0'\n","\n","#importing the display function\n","from IPython.display import display\n","\n","#importing thr treebank corpus\n","from nltk.corpus import treebank\n","\n","#parsing the file\n","tree1 = treebank.parsed_sents('wsj_0001.mrg')[0]\n","\n","#displaying the parse tree\n","display(tree1)\n"]},{"cell_type":"markdown","metadata":{"id":"7j1j-CZfQied"},"source":["b) Parse Tree 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HLnjcdnQk8q"},"outputs":[],"source":["#setting the display name and $DISPLAY environment variable \n","import os\n","os.system('Xvfb :1 -screen 0 1600x1200x16  &')\n","os.environ['DISPLAY']=':1.0'\n","\n","#importing the display function\n","from IPython.display import display\n","\n","#importing thr treebank corpus\n","from nltk.corpus import treebank\n","\n","#parsing the file\n","tree2 = treebank.parsed_sents('wsj_0002.mrg')[0]\n","\n","#displaying the parse tree\n","display(tree2)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM4a6HsdRKu38CNuNxqKdjB","collapsed_sections":[],"name":"NLP_Theory_DA (19BCT0171, 19BCT0189).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
